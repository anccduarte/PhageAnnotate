# -*- coding: utf-8 -*-

import argparse
import numpy as np
import os
from contextlib import contextmanager
from sklearn.base import BaseEstimator
from sklearn.neural_network import MLPClassifier

# FUNCTIONS
# ---

@contextmanager
def change_dir(new_path: str) -> None:
    """
    Changes directory to <new_path> within the context.

    Parameters
    ----------
    new_path: str
        The path to the target directory
    """
    cwd_path = os.path.abspath(".")
    os.chdir(new_path)
    yield
    os.chdir(cwd_path)

def get_args(*args) -> argparse.Namespace:
    """
    Wrapper for argparse.ArgumentParser. Receives a list of tuples, each tuple
    consisting of (<name_arg>, <default>) or (<name_arg>,), and passes the
    information contained within every tuple to an argument parser (instance of
    argparse.ArgumentParser). The arguments are, then, parsed (from sys.argv)
    and returned as an instance of argparse.Namespace.
    
    Parameters
    ----------
    args: list
        A list containing an arbitrary number of tuples
        (<name_arg>, <default>) / (<name_arg>,)
    """
    parser = argparse.ArgumentParser()
    for tup in args:
        name, *temp = tup
        default = temp[0] if temp else None
        parser.add_argument(name, default=default)
    args = parser.parse_args()
    return args

# CLASSES
# ---

class MLPCWrapper(BaseEstimator):
    
    """
    Wrapper class for sklearn's MLPClassifier (MLPC). It solves the problem
    generated by optimizing MLPC's "hidden_layer_sizes" with skopt's 
    BayesSearchCV. Instantiating a list of tuples of "hidden_layer_sizes"
    with skopt's Categorical produced the following error message:
    ---
    "ValueError: can only convert an array of size 1 to a Python scalar"
    ---
    Refer to the following post on Stack Overflow explaining the issue in
    more detail: https://stackoverflow.com/questions/63308474/
    """
    
    def __init__(self,
                 num_layers: int = 2,
                 activation: str = "relu",
                 solver: str = "adam",
                 learning_rate: str = "constant",
                 learning_rate_init: float = 0.001,
                 max_iter: int = 200) -> None:
        """
        Initializes an instance of MLPCWrapper.
        
        Parameters
        ----------
        num_layers: int (default=2)
            The number of hidden layers in the neural network
        activation: str (default="relu")
            Activation function for the hidden layers
        solver: str (default="adam")
            The solver for weight optimization
        learning_rate: str (default="constant")
            Learning rate schedule for weight updates
        learning_rate_init: float (default=0.001)
            The initial learning rate used
        max_iter: int (default=200)
            Maximum number of iterations
        """
        # parameters
        self.num_layers = num_layers
        self.activation = activation
        self.solver = solver
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.max_iter = max_iter
        # attributes
        self.model = None
        
    def _get_net_shape(self, x_train: np.ndarray) -> tuple:
        """
        Computes and returns "net_shape" ("hidden_layer_sizes") based on the
        attribute <self.num_layers>. The method for computing the shape of
        the neural net is deterministic and depends on the number of features
        in <x_train> and on the number of layers <self.num_layers>. Returns a
        tuple composed of powers of 2 smaller than or equal to the number of
        features in <x_train> arranged in decreasing order and evenly spaced.
        
        Parameters
        ----------
        x_train: np.ndarray
            The feature vectors fed to the MLPClassifier (in this context, it
            is only needed for determining the number of features)
        """
        # compute "net_shape" ("hidden_layer_sizes")
        num_features = x_train.shape[1]
        max_num_neurons_log2 = int(np.floor(np.log2(num_features)))
        net_shape_log2 = np.linspace(max_num_neurons_log2,
                                     2,
                                     self.num_layers,
                                     dtype=int)
        net_shape = tuple(np.power(2, net_shape_log2))
        # return "net_shape" (tuple of layer sizes)
        return net_shape
        
    def fit(self, x_train: np.ndarray, y_train: np.ndarray) -> "MLPCWrapper":
        """
        Fits an instance of MLPClassifier using the parameters passed to the
        constructor (except for "hidden_layer_sizes" which is computed by the
        auxiliary method "_get_net_shape"). Returns self.
        
        Parameters
        ----------
        x_train: np.ndarray
            The feature vectors fed to the MLPClassifier
        y_train: np.ndarray
            The label vector fed to the MLPClassifier
        """
        # compute "net_shape" ("hidden_layer_sizes")
        net_shape = self._get_net_shape(x_train=x_train)
        # fit MLPClassifier model
        model = MLPClassifier(hidden_layer_sizes=net_shape,
                              activation=self.activation,
                              solver=self.solver,
                              learning_rate=self.learning_rate,
                              learning_rate_init=self.learning_rate_init,
                              max_iter=self.max_iter).fit(x_train, y_train)
        # reassign <self.model> and return
        self.model = model
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts and returns the output of the dataset <X>.
        
        Parameters
        ----------
        X: np.ndarray
            The dataset to predict the output of
        """
        predictions = self.model.predict(X)
        return predictions

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        Computes and returns the score of the model on the given data <X> and
        corresponding true labels <y>. MLPClassifier's scoring function
        (accuracy) is utilized.
        
        X: np.ndarray
            The input data
        y: np.ndarray
            The true labels of <x>
        """
        accuracy_score = self.model.score(X, y)
        return accuracy_score
